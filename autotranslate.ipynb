{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96kvih9mXkNN"
   },
   "source": [
    "# **Videos Transcription and Translation with Faster Whisper and ChatGPT**\n",
    "\n",
    "\n",
    "[![notebook shield](https://img.shields.io/static/v1?label=&message=Notebook&color=blue&style=for-the-badge&logo=googlecolab&link=https://colab.research.google.com/github/lewangdev/autotranslate/blob/main/autotranslate.ipynb)](https://colab.research.google.com/github/lewangdev/autotranslate/blob/main/autotranslate.ipynb)\n",
    "[![repository shield](https://img.shields.io/static/v1?label=&message=Repository&color=blue&style=for-the-badge&logo=github&link=https://github.com/lewangdev/autotranslate)](https://github.com/lewangdev/autotranslate)\n",
    "\n",
    "This Notebook will guide you through the transcription and translation of video using [Faster Whisper](https://github.com/guillaumekln/faster-whisper) and ChatGPT. You'll be able to explore most inference parameters or use the Notebook as-is to store the transcript, translation and video audio in your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "QshUbLqpX7L4"
   },
   "outputs": [],
   "source": [
    "#@markdown # **Check GPU type** üïµÔ∏è\n",
    "\n",
    "#@markdown The type of GPU you get assigned in your Colab session defined the speed at which the video will be transcribed.\n",
    "#@markdown The higher the number of floating point operations per second (FLOPS), the faster the transcription.\n",
    "#@markdown But even the least powerful GPU available in Colab is able to run any Whisper model.\n",
    "#@markdown Make sure you've selected `GPU` as hardware accelerator for the Notebook (Runtime &rarr; Change runtime type &rarr; Hardware accelerator).\n",
    "\n",
    "#@markdown |  GPU   |  GPU RAM   | FP32 teraFLOPS |     Availability   |\n",
    "#@markdown |:------:|:----------:|:--------------:|:------------------:|\n",
    "#@markdown |  T4    |    16 GB   |       8.1      |         Free       |\n",
    "#@markdown | P100   |    16 GB   |      10.6      |      Colab Pro     |\n",
    "#@markdown | V100   |    16 GB   |      15.7      |  Colab Pro (Rare)  |\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown **Factory reset your Notebook's runtime if you want to get assigned a new GPU.**\n",
    "\n",
    "!nvidia-smi -L\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "IfG0E_WbRFI0"
   },
   "outputs": [],
   "source": [
    "#@markdown # **Install libraries** üèóÔ∏è\n",
    "#@markdown This cell will take a little while to download several libraries.\n",
    "\n",
    "#@markdown ---\n",
    "\n",
    "! pip install faster-whisper\n",
    "! pip install yt-dlp\n",
    "! pip install openai\n",
    "\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "from faster_whisper import WhisperModel\n",
    "from pathlib import Path\n",
    "import yt_dlp\n",
    "import subprocess\n",
    "import torch\n",
    "import shutil\n",
    "import numpy as np\n",
    "from IPython.display import display, Markdown, YouTubeVideo\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "print('Using device:', device, file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "1zwGAsr4sIgd"
   },
   "outputs": [],
   "source": [
    "#@markdown # **Optional:** Save data in Google Drive üíæ\n",
    "#@markdown Enter a Google Drive path and run this cell if you want to store the results inside Google Drive.\n",
    "\n",
    "# Uncomment to copy generated images to drive, faster than downloading directly from colab in my experience.\n",
    "from google.colab import drive\n",
    "drive_mount_path = Path(\"/\") / \"content\" / \"drive\"\n",
    "drive.mount(str(drive_mount_path))\n",
    "drive_mount_path /= \"My Drive\"\n",
    "#@markdown ---\n",
    "drive_path = \"Colab Notebooks/Videos Transcription and Translation\" #@param {type:\"string\"}\n",
    "#@markdown ---\n",
    "#@markdown **Run this cell again if you change your Google Drive path.**\n",
    "\n",
    "drive_whisper_path = drive_mount_path / Path(drive_path.lstrip(\"/\"))\n",
    "drive_whisper_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "TMhrSq_GZ6kA"
   },
   "outputs": [],
   "source": [
    "#@markdown # **Model selection** üß†\n",
    "\n",
    "#@markdown As of the first public release, there are 4 pre-trained options to play with:\n",
    "\n",
    "#@markdown |  Size  | Parameters | English-only model | Multilingual model | Required VRAM | Relative speed |\n",
    "#@markdown |:------:|:----------:|:------------------:|:------------------:|:-------------:|:--------------:|\n",
    "#@markdown |  tiny  |    39 M    |     `tiny.en`      |       `tiny`       |     ~0.8 GB     |      ~32x      |\n",
    "#@markdown |  base  |    74 M    |     `base.en`      |       `base`       |     ~1.0 GB     |      ~16x      |\n",
    "#@markdown | small  |   244 M    |     `small.en`     |      `small`       |     ~1.4 GB     |      ~6x       |\n",
    "#@markdown | medium |   769 M    |    `medium.en`     |      `medium`      |     ~2.7 GB     |      ~2x       |\n",
    "#@markdown | large-v1  |   1550 M   |        N/A         |      `large-v1`       |    ~4.3 GB     |       1x       |\n",
    "#@markdown | large-v2  |   1550 M   |        N/A         |      `large-v2`       |    ~4.3 GB     |       1x       |\n",
    "\n",
    "#@markdown ---\n",
    "model_size = 'large-v2' #@param ['tiny', 'tiny.en', 'base', 'base.en', 'small', 'small.en', 'medium', 'medium.en', 'large-v1', 'large-v2']\n",
    "device_type = \"cuda\" #@param {type:\"string\"} ['cuda', 'cpu']\n",
    "compute_type = \"float16\" #@param {type:\"string\"} ['float16', 'int8_float16', 'int8']\n",
    "#@markdown ---\n",
    "#@markdown **Run this cell again if you change the model.**\n",
    "\n",
    "model = WhisperModel(model_size, device=device_type, compute_type=compute_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "xYLPZQX9S7tU"
   },
   "outputs": [],
   "source": [
    "#@markdown # **Video selection** üì∫\n",
    "\n",
    "#@markdown Enter the URL of the video you want to transcribe, wether you want to save the audio file in your Google Drive, and run the cell.\n",
    "\n",
    "Type = \"Video or playlist URL\" #@param ['Video or playlist URL', 'Google Drive']\n",
    "#@markdown ---\n",
    "#@markdown #### **Video or playlist URL**\n",
    "URL = \"https://dft3h5i221ap1.cloudfront.net/OpenAI/c2/video/sc-openai-c2-L5-vid6_2.mp4\" #@param {type:\"string\"}\n",
    "# store_audio = True #@param {type:\"boolean\"}\n",
    "#@markdown ---\n",
    "#@markdown #### **Google Drive video, audio (mp4, wav), or folder containing video and/or audio files**\n",
    "video_path = \"Colab Notebooks/transcription/my_video.mp4\" #@param {type:\"string\"}\n",
    "#@markdown ---\n",
    "#@markdown **Run this cell again if you change the video.**\n",
    "\n",
    "video_path_local_list = []\n",
    "\n",
    "if Type == \"Video or playlist URL\":\n",
    "    \n",
    "    ydl_opts = {\n",
    "        'format': 'm4a/bestaudio/best',\n",
    "        'outtmpl': '%(id)s.%(ext)s',\n",
    "        # ‚ÑπÔ∏è See help(yt_dlp.postprocessor) for a list of available Postprocessors and their arguments\n",
    "        'postprocessors': [{  # Extract audio using ffmpeg\n",
    "            'key': 'FFmpegExtractAudio',\n",
    "            'preferredcodec': 'wav',\n",
    "        }]\n",
    "    }\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        error_code = ydl.download([URL])\n",
    "        list_video_info = [ydl.extract_info(URL, download=False)]\n",
    "        \n",
    "    for video_info in list_video_info:\n",
    "        video_path_local_list.append(Path(f\"{video_info['id']}.wav\"))\n",
    "\n",
    "elif Type == \"Google Drive\":\n",
    "    # video_path_drive = drive_mount_path / Path(video_path.lstrip(\"/\"))\n",
    "    video_path = drive_mount_path / Path(video_path.lstrip(\"/\"))\n",
    "    if video_path.is_dir():\n",
    "        for video_path_drive in video_path.glob(\"**/*\"):\n",
    "            if video_path_drive.is_file():\n",
    "                display(Markdown(f\"**{str(video_path_drive)} selected for transcription.**\"))\n",
    "            elif video_path_drive.is_dir():\n",
    "                display(Markdown(f\"**Subfolders not supported.**\"))\n",
    "            else:\n",
    "                display(Markdown(f\"**{str(video_path_drive)} does not exist, skipping.**\"))\n",
    "            video_path_local = Path(\".\").resolve() / (video_path_drive.name)\n",
    "            shutil.copy(video_path_drive, video_path_local)\n",
    "            video_path_local_list.append(video_path_local)\n",
    "    elif video_path.is_file():\n",
    "        video_path_local = Path(\".\").resolve() / (video_path.name)\n",
    "        shutil.copy(video_path, video_path_local)\n",
    "        video_path_local_list.append(video_path_local)\n",
    "        display(Markdown(f\"**{str(video_path)} selected for transcription.**\"))\n",
    "    else:\n",
    "        display(Markdown(f\"**{str(video_path)} does not exist.**\"))\n",
    "\n",
    "else:\n",
    "    raise(TypeError(\"Please select supported input type.\"))\n",
    "\n",
    "for video_path_local in video_path_local_list:\n",
    "    if video_path_local.suffix == \".mp4\":\n",
    "        video_path_local = video_path_local.with_suffix(\".wav\")\n",
    "        result  = subprocess.run([\"ffmpeg\", \"-i\", str(video_path_local.with_suffix(\".mp4\")), \"-vn\", \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \"-ac\", \"1\", str(video_path_local)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "collapsed": true,
    "id": "Ad6n1m4deAHp",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#@markdown # **Run the model** üöÄ\n",
    "\n",
    "#@markdown Run this cell to execute the transcription of the video. This can take a while and very based on the length of the video and the number of parameters of the model selected above.\n",
    "def seconds_to_time_format(s):\n",
    "    # Convert seconds to hours, minutes, seconds, and milliseconds\n",
    "    hours = s // 3600\n",
    "    s %= 3600\n",
    "    minutes = s // 60\n",
    "    s %= 60\n",
    "    seconds = s // 1\n",
    "    milliseconds = round((s % 1) * 1000)\n",
    "    \n",
    "    # Return the formatted string\n",
    "    return f\"{int(hours):02d}:{int(minutes):02d}:{int(seconds):02d},{int(milliseconds):03d}\"\n",
    "\n",
    "\n",
    "#@markdown ## **Parameters** ‚öôÔ∏è\n",
    "\n",
    "#@markdown ### **Behavior control**\n",
    "#@markdown #### Language\n",
    "language = \"en\" #@param [\"auto\", \"en\", \"zh\", \"ja\", \"fr\", \"de\"] {allow-input: true}\n",
    "#@markdown #### initial prompt\n",
    "initial_prompt = \"Hello, Let's begin to talk.\" #@param {type:\"string\"}\n",
    "#@markdown ---\n",
    "#@markdown #### Word-level timestamps\n",
    "word_level_timestamps = True #@param {type:\"boolean\"}\n",
    "#@markdown ---\n",
    "#@markdown #### VAD filter\n",
    "vad_filter = False #@param {type:\"boolean\"}\n",
    "vad_filter_min_silence_duration_ms = 50 #@param {type:\"integer\"}\n",
    "#@markdown ---\n",
    "\n",
    "\n",
    "segments, info = model.transcribe(str(video_path_local), beam_size=5,\n",
    "                                  language=None if language == \"auto\" else language,\n",
    "                                  initial_prompt=initial_prompt,\n",
    "                                  word_timestamps=word_level_timestamps, \n",
    "                                  vad_filter=vad_filter,\n",
    "                                  vad_parameters=dict(min_silence_duration_ms=vad_filter_min_silence_duration_ms))\n",
    "\n",
    "display(Markdown(f\"Detected language '{info.language}' with probability {info.language_probability}\"))\n",
    "\n",
    "fragments = []\n",
    "\n",
    "for segment in segments:\n",
    "  print(f\"[{seconds_to_time_format(segment.start)} --> {seconds_to_time_format(segment.end)}] {segment.text}\")\n",
    "  if word_level_timestamps:\n",
    "    for word in segment.words:\n",
    "      ts_start = seconds_to_time_format(word.start)\n",
    "      ts_end = seconds_to_time_format(word.end)\n",
    "      #print(f\"[{ts_start} --> {ts_end}] {word.word}\")\n",
    "      fragments.append(dict(start=word.start,end=word.end,text=word.word))\n",
    "  else:\n",
    "    ts_start = seconds_to_time_format(segment.start)\n",
    "    ts_end = seconds_to_time_format(segment.end)\n",
    "    #print(f\"[{ts_start} --> {ts_end}] {segment.text}\")\n",
    "    fragments.append(dict(start=segment.start,end=segment.end,text=segment.text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "v9214wd0Nk5J"
   },
   "outputs": [],
   "source": [
    "#@title Merge words/segments to sentences\n",
    "\n",
    "#@markdown Run this cell to merge words/segments to sentences.\n",
    "#@markdown ## **Parameters** ‚öôÔ∏è\n",
    "\n",
    "#@markdown ### **Behavior control**\n",
    "#@markdown #### Milliseconds gap between_two sentences\n",
    "max_gap_ms_between_two_sentence = 200 #@param {type:\"integer\"}\n",
    "\n",
    "import json\n",
    "\n",
    "# Merge words/segments to sentences\n",
    "def merge_fragments(fragments, gap_ms):\n",
    "  new_fragments = []\n",
    "  new_fragment = {}\n",
    "  length = len(fragments)\n",
    "  for i, fragment in enumerate(fragments):\n",
    "    start = fragment['start']\n",
    "    end = fragment['end']\n",
    "    text = fragment['text']\n",
    "\n",
    "    if new_fragment.get('start', None) is None:\n",
    "      new_fragment['start'] = start\n",
    "    if new_fragment.get('end', None) is None:\n",
    "      new_fragment['end'] = end\n",
    "    if new_fragment.get('text', None) is None:\n",
    "      new_fragment['text'] = \"\"\n",
    "\n",
    "    if start - new_fragment['end'] > gap_ms:\n",
    "      new_fragments.append(new_fragment)\n",
    "      new_fragment = dict(start=start, end=end, text=text)\n",
    "      continue\n",
    "\n",
    "    new_fragment['end'] = end\n",
    "\n",
    "    delimiter = '' if text.startswith('-') else ' '  \n",
    "    new_fragment['text'] = f\"{new_fragment['text']}{delimiter}{text.lstrip()}\"\n",
    "\n",
    "    # End of a sentence when symbols found: [.?]\n",
    "    if text.endswith('.') or text.endswith('?') or i == length-1:\n",
    "      new_fragments.append(new_fragment)\n",
    "      new_fragment = {}\n",
    "  return new_fragments\n",
    "\n",
    "\n",
    "new_fragments = merge_fragments(fragments, max_gap_ms_between_two_sentence/1000.0)\n",
    "\n",
    "# Save as json file\n",
    "json_ext_name = \".json\"\n",
    "json_transcript_file_name = video_path_local.stem + json_ext_name\n",
    "with open(json_transcript_file_name, 'w') as f:\n",
    "  f.write(json.dumps(new_fragments))\n",
    "display(Markdown(f\"**Transcript SRT file created: {video_path_local.parent / json_transcript_file_name}**\"))\n",
    "\n",
    "# Save as srt\n",
    "srt_ext_name = \".srt\"\n",
    "srt_transcript_file_name = video_path_local.stem + srt_ext_name\n",
    "with open(srt_transcript_file_name, 'w') as f:\n",
    "  for sentence_idx, fragment in enumerate(new_fragments):\n",
    "    ts_start = seconds_to_time_format(fragment['start'])\n",
    "    ts_end = seconds_to_time_format(fragment['end'])\n",
    "    text = fragment['text']\n",
    "    print(f\"[{ts_start} --> {ts_end}] {text}\")\n",
    "    f.write(f\"{sentence_idx + 1}\\n\")\n",
    "    f.write(f\"{ts_start} --> {ts_end}\\n\")\n",
    "    f.write(f\"{text.strip()}\\n\\n\")\n",
    "\n",
    "try:\n",
    "  shutil.copy(video_path_local.parent / srt_transcript_file_name,\n",
    "            drive_whisper_path / srt_transcript_file_name\n",
    "  )\n",
    "  display(Markdown(f\"**Transcript SRT file created: {drive_whisper_path / srt_transcript_file_name}**\"))\n",
    "except:\n",
    "  display(Markdown(f\"**Transcript SRT file created: {video_path_local.parent / srt_transcript_file_name}**\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "collapsed": true,
    "id": "L3lwd7ZF1SRX",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#@markdown # **Translate**\n",
    "#@markdown Run this cell to translate subtitles to the language you want.\n",
    "#@markdown ## **Parameters** ‚öôÔ∏è\n",
    "\n",
    "#@markdown ### **Behavior control**\n",
    "\n",
    "#@markdown #### API Type\n",
    "api_type = \"azure\" #@param [\"azure\", \"openai\"]\n",
    "\n",
    "#@markdown #### Azure API ConfigÔºàIf you are using `openai`, please leave these fields blank.Ôºâ\n",
    "api_base = \"https://xxxxxx.openai.azure.com\" #@param {type:\"string\"}\n",
    "api_version = \"2023-05-15\" #@param {type:\"string\"}\n",
    "deployment_id = \"gpt3\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown #### API Key and Model Config\n",
    "api_key = \"xxxxx\" #@param {type:\"string\"}\n",
    "model_name = \"gpt-3.5-turbo\" #@param [\"gpt-3.5-turbo\"] {allow-input: true}\n",
    "temperature = 0 #@param {type:\"number\"}\n",
    "#@markdown ---\n",
    "#@markdown #### Target Language\n",
    "target_language = \"\\u7B80\\u4F53\\u4E2D\\u6587\" #@param [\"\\u7B80\\u4F53\\u4E2D\\u6587\", \"\\u7E41\\u9AD4\\u4E2D\\u6587\", \"\\u65E5\\u672C\\u8A9E\"] {allow-input: true}\n",
    "#@markdown ---\n",
    "#@markdown #### Retry and Token Chunks\n",
    "translate_max_retry_times = 10 #@param {type:\"integer\"}\n",
    "count_of_sentence_send_once_limit = 5 #@param {type:\"integer\"}\n",
    "\n",
    "# This prompt is from https://twitter.com/dotey/status/1665476562219573249\n",
    "system_prompt = f\"\"\"You are a program responsible for translating subtitles. Your task is to translate the subtitles into {target_language}, maintaining a colloquial tone and style, avoiding long sentences, and ignoring verbal tics such as 'so', 'you know', etc.\n",
    "The input will be a JSON-formatted string array, which should be translated in accordance with the following steps:\n",
    "Step1: Join the string array to a sentence, then translate it to {target_language};\n",
    "Step2: Split the translated sentence to a string array, each item of which should correspond to an item in the original input array.\n",
    "Step3: Verify if the count of items in the output array equals that of the input array and no item is blank. If it doesn't, go back to Step 2 and try again.\n",
    "  \n",
    "Respond with a JSON-formatted string array:\n",
    "\"\"\"\n",
    "import openai\n",
    "import json\n",
    "\n",
    "openai.api_key = api_key\n",
    "\n",
    "if api_type == \"azure\":\n",
    "  openai.api_type = \"azure\"\n",
    "  openai.api_base = api_base\n",
    "  openai.api_version = api_version\n",
    "else:\n",
    "  deployment_id = None\n",
    "\n",
    "\n",
    "def translate_by_chatgpt(sentences, max_retry_times=10, deployment_id=None, model_name=\"gpt-3.5-turbo\", temperature=0.7):\n",
    "  system_msg = dict(role=\"system\", content=system_prompt)\n",
    "  user_msg_content = json.dumps(sentences)\n",
    "  user_msg = dict(role=\"user\", content=user_msg_content)\n",
    "  current_retry_times = 0\n",
    "\n",
    "  while True:\n",
    "    try:\n",
    "      chat_completion = openai.ChatCompletion.create(deployment_id=deployment_id, \n",
    "                                                     model=model_name, \n",
    "                                                     messages=[system_msg, user_msg],\n",
    "                                                     temperature=temperature)\n",
    "      sentences_translated = json.loads(chat_completion.choices[0].message.content)\n",
    "\n",
    "      if len(sentences_translated) != len(sentences) and current_retry_times < max_retry_times:\n",
    "        current_retry_times = current_retry_times + 1\n",
    "        print(f\"==Tranlate Retry with {current_retry_times} times, Reason: translated={len(sentences_translated)}, origin={len(sentences)}\")\n",
    "        continue\n",
    "      \n",
    "      break\n",
    "    except:\n",
    "      if current_retry_times >= max_retry_times:\n",
    "        break\n",
    "      current_retry_times = current_retry_times + 1\n",
    "      print(f\"==Tranlate Retry with {current_retry_times} times\")\n",
    "      continue\n",
    "  return sentences_translated\n",
    "\n",
    "def translate_fragments(fragments, sentence_send_limit=5):\n",
    "  system_msg = dict(role=\"system\", content=system_prompt)\n",
    "  fragments_translated = []\n",
    "\n",
    "  # Todo: The count of tokens in sentences must be less than Max Tokens API allowed\n",
    "  length = len(fragments)\n",
    "  for n in range(0, length, sentence_send_limit):\n",
    "    fragments_will_be_translated = fragments[n:n+sentence_send_limit]\n",
    "    sentences_translated = translate_by_chatgpt(list(map(lambda x: x['text'], fragments_will_be_translated)), \n",
    "                                                translate_max_retry_times,\n",
    "                                                deployment_id,\n",
    "                                                model_name)\n",
    "\n",
    "    for i, sentence_translated in enumerate(sentences_translated):\n",
    "      print(f\"{seconds_to_time_format(fragments_will_be_translated[i]['start'])} --> {seconds_to_time_format(fragments_will_be_translated[i]['end'])}\")\n",
    "      print(\"Original  : \" + fragments_will_be_translated[i]['text'].lstrip())\n",
    "      print(\"Translated: \" + sentence_translated)\n",
    "      print('\\n')\n",
    "      fragments_will_be_translated[i]['text_translated'] = sentence_translated\n",
    "    \n",
    "    fragments_translated.extend(fragments_will_be_translated)\n",
    "  \n",
    "  return fragments_translated\n",
    "\n",
    "fragments_translated = translate_fragments(new_fragments, count_of_sentence_send_once_limit)\n",
    "\n",
    "# Save translation as json file\n",
    "json_translated_file_name = f\"{video_path_local.stem}-translated.json\"\n",
    "with open(json_translated_file_name, 'w') as f:\n",
    "  f.write(json.dumps(new_fragments))\n",
    "display(Markdown(f\"**Translation JSON file created: {video_path_local.parent / json_translated_file_name}**\"))\n",
    "\n",
    "# Save translation as srt file\n",
    "srt_translated_file_name = f\"{video_path_local.stem}-translated.srt\"\n",
    "with open(srt_translated_file_name, 'w') as f:\n",
    "  for sentence_idx, fragment in enumerate(fragments_translated):\n",
    "    ts_start = seconds_to_time_format(fragment['start'])\n",
    "    ts_end = seconds_to_time_format(fragment['end'])\n",
    "    text = fragment.get('text', '')\n",
    "    text_translated = fragment.get('text_translated', '')\n",
    "    f.write(f\"{sentence_idx + 1}\\n\")\n",
    "    f.write(f\"{ts_start} --> {ts_end}\\n\")\n",
    "    f.write(f\"{text_translated.strip()}\\n\")\n",
    "    f.write(f\"{text.strip()}\\n\\n\")\n",
    "\n",
    "try:\n",
    "  shutil.copy(video_path_local.parent / srt_translated_file_name,\n",
    "            drive_whisper_path / srt_translated_file_name\n",
    "  )\n",
    "  display(Markdown(f\"**Translated SRT file created: {drive_whisper_path / srt_translated_file_name}**\"))\n",
    "except:\n",
    "  display(Markdown(f\"**Translated SRT file created: {video_path_local.parent / srt_translated_file_name}**\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid audio file: output/audio_792.56.wav\n"
     ]
    }
   ],
   "source": [
    "# ÂØºÂÖ•ÊâÄÈúÄÁöÑÂ∫ì\n",
    "import edge_tts\n",
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "import nest_asyncio\n",
    "\n",
    "# ÂÆö‰πâÂÖ®Â±ÄÂèòÈáèÔºåÂÜ≥ÂÆöÊòØÂê¶Âú®Èü≥È¢ëÈó¥Â°´ÂÖÖÈùôÈªò\n",
    "FILL_SILENCE = False\n",
    "# ÂÆö‰πâÂÖ®Â±ÄÂèòÈáèÔºåËÆæÁΩÆÂ§ÑÁêÜÁöÑÂ≠óÂπïÊï∞ÈáèÔºå-1Ë°®Á§∫Â§ÑÁêÜÊâÄÊúâÂ≠óÂπï\n",
    "NUM_SUBTITLES_TO_PROCESS = -1\n",
    "# ÂÆö‰πâÂÖ®Â±ÄÂèòÈáèÔºåËÆæÁΩÆÂπ∂Âèë‰ªªÂä°ÁöÑÊúÄÂ§ßÊï∞Èáè\n",
    "MAX_CONCURRENT_TASKS = 5\n",
    "\n",
    "# ÂÆö‰πâÂáΩÊï∞ÔºåËé∑Âèñ‰∏ã‰∏Ä‰∏™Â≠óÂπïÁöÑÂºÄÂßãÊó∂Èó¥\n",
    "def get_next_start(audio_file, data):\n",
    "    # ‰ªéÊñá‰ª∂Âêç‰∏≠Ëé∑ÂèñÂΩìÂâçÂ≠óÂπïÁöÑÂºÄÂßãÊó∂Èó¥\n",
    "    current_start = float(audio_file.split(\n",
    "        \"/\")[-1].replace(\"audio_\", \"\").replace(\".wav\", \"\"))\n",
    "    next_start = None\n",
    "    # ÈÅçÂéÜÂ≠óÂπïÊï∞ÊçÆÔºåÊâæÂà∞‰∏ã‰∏Ä‰∏™ÂºÄÂßãÊó∂Èó¥\n",
    "    for subtitle in data:\n",
    "        if subtitle['start'] > current_start:\n",
    "            next_start = subtitle['start']\n",
    "            break\n",
    "    return next_start\n",
    "\n",
    "# ÂÆö‰πâÂºÇÊ≠•ÂáΩÊï∞ÔºåËΩ¨Êç¢Â≠óÂπï‰∏∫ËØ≠Èü≥\n",
    "async def process_subtitle(semaphore, subtitle):\n",
    "    async with semaphore:  # ‰ΩøÁî®‰ø°Âè∑ÈáèÈôêÂà∂Âπ∂Âèë‰ªªÂä°Êï∞Èáè\n",
    "        # ËÆæÁΩÆËæìÂá∫ÁõÆÂΩï\n",
    "        output_dir = \"output\"\n",
    "        # Â¶ÇÊûúËæìÂá∫ÁõÆÂΩï‰∏çÂ≠òÂú®ÔºåÂàôÂàõÂª∫\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        # ÂÆö‰πâËæìÂá∫ÁöÑÈü≥È¢ëÊñá‰ª∂Âêç\n",
    "        audio_file = os.path.join(\n",
    "            output_dir, \"audio_\" + str(subtitle['start']) + \".wav\")\n",
    "\n",
    "        # Â¶ÇÊûúÈü≥È¢ëÊñá‰ª∂Â∑≤Â≠òÂú®ÔºåÂàôË∑≥Ëøá\n",
    "        if os.path.exists(audio_file):\n",
    "            print(f\"Skipping {audio_file} because it already exists.\")\n",
    "            return audio_file\n",
    "\n",
    "        # ‰ΩøÁî®edge_ttsÂ∫ìÂ∞ÜÂ≠óÂπïÊñáÊú¨ËΩ¨Êç¢‰∏∫ËØ≠Èü≥ÔºåÂπ∂‰øùÂ≠ò‰∏∫Èü≥È¢ëÊñá‰ª∂\n",
    "        communicate = edge_tts.Communicate(\n",
    "            subtitle['text_translated'], voice='zh-CN-XiaoxiaoNeural', rate='+75%')\n",
    "        await communicate.save(audio_file)\n",
    "\n",
    "        # Ê£ÄÊü•ÁîüÊàêÁöÑÈü≥È¢ëÊñá‰ª∂ÊòØÂê¶ÊúâÊïà\n",
    "        try:\n",
    "            AudioSegment.from_file(audio_file)\n",
    "        except:\n",
    "            print(f\"Invalid audio file generated: {audio_file}\")\n",
    "            return None\n",
    "\n",
    "    return audio_file  # ÈÄÄÂá∫'with'ÂùóÊó∂ÔºåËá™Âä®ÈáäÊîæ‰ø°Âè∑Èáè\n",
    "\n",
    "# ÂÆö‰πâÂáΩÊï∞ÔºåÂ∞ÜÊâÄÊúâÈü≥È¢ëÊñá‰ª∂ËøûÊé•Ëµ∑Êù•ÔºåÂ¶ÇÊûúÈúÄË¶ÅÔºå‰∏≠Èó¥ÂèØ‰ª•ÊèíÂÖ•ÈùôÈªò\n",
    "def concatenate_audios(audio_files, data):\n",
    "    # ÂàõÂª∫‰∏Ä‰∏™Á©∫ÁöÑÈü≥È¢ëÊÆµ\n",
    "    combined = AudioSegment.empty()\n",
    "    # ÈÅçÂéÜÈü≥È¢ëÊñá‰ª∂\n",
    "    for audio_file in audio_files:\n",
    "        # Â¶ÇÊûúÈü≥È¢ëÊñá‰ª∂Êó†ÊïàÔºåÂàôË∑≥Ëøá\n",
    "        if audio_file is None:\n",
    "            continue\n",
    "\n",
    "        # Â∞ùËØï‰ªéÈü≥È¢ëÊñá‰ª∂ËØªÂèñÈü≥È¢ëÊï∞ÊçÆ\n",
    "        try:\n",
    "            audio = AudioSegment.from_file(audio_file)\n",
    "        except:\n",
    "            print(f\"Invalid audio file: {audio_file}\")\n",
    "            continue\n",
    "\n",
    "        # Â∞ÜÈü≥È¢ëÊï∞ÊçÆÊ∑ªÂä†Âà∞ÊÄªÈü≥È¢ëÊÆµ\n",
    "        combined += audio\n",
    "        # Â¶ÇÊûúÈúÄË¶ÅÂú®Èü≥È¢ëÈó¥Â°´ÂÖÖÈùôÈªòÔºåÂàôÊ∑ªÂä†ÈùôÈªò\n",
    "        if FILL_SILENCE:\n",
    "            next_start = get_next_start(audio_file, data)\n",
    "            silence_duration = next_start - len(audio) / 1000.0\n",
    "            if silence_duration > 0:\n",
    "                silence = AudioSegment.silent(duration=silence_duration * 1000)\n",
    "                combined += silence\n",
    "    return combined\n",
    "\n",
    "# ÂÆö‰πâ‰∏ªÂáΩÊï∞ÔºåÂ§ÑÁêÜÊâÄÊúâÂ≠óÂπïÂπ∂ÁîüÊàêÊúÄÁªàÈü≥È¢ë\n",
    "async def main():\n",
    "    # ËØªÂèñÂ≠óÂπïÊï∞ÊçÆÊñá‰ª∂\n",
    "    with open('1680627742235148292-translated.json') as f:\n",
    "        data = json.load(f)\n",
    "    # ÂàõÂª∫‰∏Ä‰∏™‰ø°Âè∑ÈáèÔºåÈôêÂà∂Âπ∂Âèë‰ªªÂä°ÁöÑÊï∞Èáè\n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENT_TASKS)\n",
    "    # Ëé∑ÂèñÈúÄË¶ÅÂ§ÑÁêÜÁöÑÂ≠óÂπïÊï∞Èáè\n",
    "    num_subtitles_to_process = len(\n",
    "        data) if NUM_SUBTITLES_TO_PROCESS == -1 else NUM_SUBTITLES_TO_PROCESS\n",
    "    # ‰∏∫ÊØè‰∏™Â≠óÂπïÂàõÂª∫‰∏Ä‰∏™‰ªªÂä°\n",
    "    tasks = [process_subtitle(semaphore, subtitle)\n",
    "             for subtitle in data[:num_subtitles_to_process]]\n",
    "    # Âπ∂ÂèëÊâßË°åÊâÄÊúâ‰ªªÂä°ÔºåÂπ∂Ëé∑ÂèñÁªìÊûú\n",
    "    audio_files = await asyncio.gather(*tasks)\n",
    "    # Â∞ÜÊâÄÊúâÈü≥È¢ëÊñá‰ª∂ËøûÊé•Ëµ∑Êù•\n",
    "    combined = concatenate_audios(audio_files, data)\n",
    "    # ÂØºÂá∫ÊúÄÁªàÈü≥È¢ë\n",
    "    combined.export(\"final_audio.wav\", format='wav')\n",
    "\n",
    "# ‰ΩøÁî®nest_asyncioÂ∫ìÂÖÅËÆ∏ÂµåÂ•ó‰ΩøÁî®asyncioÁöÑ‰∫ã‰ª∂Âæ™ÁéØ\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Ëé∑ÂèñÂΩìÂâç‰∫ã‰ª∂Âæ™ÁéØÔºåÂπ∂ËøêË°å‰∏ªÂáΩÊï∞\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(main())\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
